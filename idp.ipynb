{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBXTeO2uE+R7qbd+ZnrR5e"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GXzzy1tC8Dt"
      },
      "outputs": [],
      "source": [
        "!pip install -U boto3 langchain langchain-pinecone langchain-community nltk sentence-transformers -U langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import boto3\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone as PineconeClient\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "kApbgSEIDL64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AWS Credentials\n",
        "AWS_ACCESS_KEY_ID=''  # Replace with your AWS Access Key ID\n",
        "AWS_SECRET_ACCESS_KEY=''  # Replace with your AWS Secret Access Key\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_REGION=''  # Set the AWS region (default: us-east-1)\n",
        "\n",
        "# Pinecone API Key and Index\n",
        "PINECONE_API_KEY=''  # Replace with your Pinecone API Key\n",
        "PINECONE_INDEX=''  # Replace with your Pinecone Index Name\n",
        "\n",
        "# Amazon S3 Bucket and File Details\n",
        "S3_BUCKET_NAME=''  # Replace with your S3 bucket name where the document is stored\n",
        "PDF_FILE_NAME=''  # Replace with the filename of the document to process\n"
      ],
      "metadata": {
        "id": "cCNuOAArDgoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AWS Textract client\n",
        "client = boto3.client(\n",
        "    'textract',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "    region_name=AWS_REGION\n",
        ")\n",
        "\n",
        "# Start document text detection\n",
        "response = client.start_document_text_detection(\n",
        "    DocumentLocation={\"S3Object\": {\"Bucket\": S3_BUCKET_NAME, \"Name\": PDF_FILE_NAME}}\n",
        ")\n",
        "job_id = response[\"JobId\"]\n",
        "print(f\"Job started with Job ID: {job_id}\")\n",
        "\n",
        "# Polling for job completion\n",
        "while True:\n",
        "    result = client.get_document_text_detection(JobId=job_id)\n",
        "    status = result[\"JobStatus\"]\n",
        "\n",
        "    if status in [\"SUCCEEDED\", \"FAILED\"]:\n",
        "        break\n",
        "\n",
        "    print(\"Processing...\")\n",
        "    time.sleep(5)\n",
        "\n",
        "if status == \"FAILED\":\n",
        "    raise Exception(\"Textract job failed!\")\n",
        "\n",
        "print(\"Processing completed!\")"
      ],
      "metadata": {
        "id": "xowCj9k7EAqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Text from Response\n",
        "extracted_text = []\n",
        "while True:\n",
        "    if \"Blocks\" in result:\n",
        "        for block in result[\"Blocks\"]:\n",
        "            if block[\"BlockType\"] == \"LINE\" and \"Text\" in block:\n",
        "                extracted_text.append(block[\"Text\"])\n",
        "\n",
        "    if \"NextToken\" in result:\n",
        "        result = client.get_document_text_detection(JobId=job_id, NextToken=result[\"NextToken\"])\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Combine extracted text into a single string\n",
        "full_text = \"\\n\".join(extracted_text)\n",
        "\n",
        "# Save extracted text to a file\n",
        "output_file_name = \"extracted_text.txt\"\n",
        "with open(output_file_name, \"w\") as output_file_io:\n",
        "    output_file_io.write(full_text)\n",
        "\n",
        "# NLP Preprocessing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization\n",
        "    tokens = [word for word in tokens if word.isalnum()]  # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Stopword removal\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "preprocessed_text = preprocess_text(full_text)\n",
        "\n",
        "# Prepare Document for Embedding\n",
        "docs = [Document(page_content=preprocessed_text)]\n",
        "\n",
        "# Split document into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1200, chunk_overlap=250, separator=\"\\n\")\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# Use Embeddings for Text Processing\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "docsearch = PineconeVectorStore.from_documents(split_docs, embedding_model, index_name=PINECONE_INDEX)\n",
        "\n",
        "print(\"Processing complete!\")"
      ],
      "metadata": {
        "id": "TAr3SGiTGTCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation history\n",
        "chat_history = []\n",
        "\n",
        "# Prompt template\n",
        "RAG_PROMPT_TEMPLATE = '''\n",
        "You are a helpful and knowledgeable AI assistant having a conversation with a user.\n",
        "Use the context and conversation history to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "You are a helpful and knowledgeable AI assistant. Use the provided context to answer the question.\n",
        "\n",
        "If the context is insufficient, rely on your own knowledge to provide the best possible response.\n",
        "\n",
        "Conversation History:\n",
        "{history}\n",
        "\n",
        "Question: {human_input}\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "PROMPT = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
        "\n",
        "# Bedrock model\n",
        "boto3_bedrock = boto3.client(\n",
        "    'bedrock-runtime',\n",
        "    region_name='us-east-1',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XthEQVf8HAMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence transformer model for evaluation\n",
        "eval_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def evaluate_response(query, response, context):\n",
        "    q_emb = eval_model.encode(query, convert_to_tensor=True)\n",
        "    r_emb = eval_model.encode(response, convert_to_tensor=True)\n",
        "    c_emb = eval_model.encode(context, convert_to_tensor=True)\n",
        "    fluency = len([w for w in word_tokenize(response) if w.isalpha()])\n",
        "    return {\n",
        "        \"query_similarity\": float(util.cos_sim(q_emb, r_emb)),\n",
        "        \"context_similarity\": float(util.cos_sim(c_emb, r_emb)),\n",
        "        \"fluency\": \"High\" if fluency > 10 else \"Low\",\n",
        "        \"fluency_score\": fluency / 100\n",
        "    }\n",
        "\n",
        "def scoring_fn(metrics):\n",
        "    return 0.4 * metrics[\"query_similarity\"] + 0.4 * metrics[\"context_similarity\"] + 0.2 * metrics[\"fluency_score\"]"
      ],
      "metadata": {
        "id": "qNtbpA0BwIwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run conversation loop\n",
        "while True:\n",
        "    human_input = input(\"\\nAsk a question (or type 'exit' to quit): \")\n",
        "    if human_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    query_embedding = embedding_model.embed_query(human_input)\n",
        "    search_results = docsearch.similarity_search(human_input, k=5)\n",
        "\n",
        "    # Create context from retrieved documents\n",
        "    MAX_CONTEXT_LENGTH = 6000\n",
        "    context_string = '\\n\\n'.join(\n",
        "        [f'Document {ind+1}: ' + i.page_content[:MAX_CONTEXT_LENGTH] for ind, i in enumerate(search_results)]\n",
        "    )\n",
        "\n",
        "    # Build conversation history\n",
        "    formatted_history = \"\"\n",
        "    for turn in chat_history:\n",
        "        formatted_history += f\"User: {turn['question']}\\nAssistant: {turn['answer']}\\n\"\n",
        "\n",
        "    prompt_data = PROMPT.format(\n",
        "        human_input=human_input,\n",
        "        context=context_string,\n",
        "        history=formatted_history\n",
        "    )\n",
        "\n",
        "    # Prepare body for both models\n",
        "    body_part = json.dumps({\n",
        "        'inputText': prompt_data,\n",
        "        'textGenerationConfig': {\n",
        "            'maxTokenCount': 3072,\n",
        "            'stopSequences': [],\n",
        "            'temperature': 0.7,\n",
        "            'topP': 1\n",
        "        }\n",
        "    })\n",
        "\n",
        "    express_response = boto3_bedrock.invoke_model(\n",
        "        body=body_part,\n",
        "        contentType=\"application/json\",\n",
        "        accept=\"application/json\",\n",
        "        modelId='amazon.titan-text-express-v1'\n",
        "    )\n",
        "    express_text = json.loads(express_response['body'].read())['results'][0]['outputText'].strip()\n",
        "\n",
        "    premier_response = boto3_bedrock.invoke_model(\n",
        "        body=body_part,\n",
        "        contentType=\"application/json\",\n",
        "        accept=\"application/json\",\n",
        "        modelId='amazon.titan-text-premier-v1:0'\n",
        "    )\n",
        "    premier_text = json.loads(premier_response['body'].read())['results'][0]['outputText'].strip()\n",
        "\n",
        "    # Evaluate and choose best\n",
        "    eval_express = evaluate_response(human_input, express_text, context_string)\n",
        "    eval_premier = evaluate_response(human_input, premier_text, context_string)\n",
        "\n",
        "    score_express = scoring_fn(eval_express)\n",
        "    score_premier = scoring_fn(eval_premier)\n",
        "\n",
        "    best_model = \"Express\" if score_express >= score_premier else \"Premier\"\n",
        "    best_text = express_text if score_express >= score_premier else premier_text\n",
        "    best_text = best_text.replace(\". \", \".\\n\")\n",
        "    best_eval = eval_express if score_express >= score_premier else eval_premier\n",
        "\n",
        "    print(f\"\\nAnswer from {best_model}:\\n{best_text}\")\n",
        "    print(f\"\\n[Evaluation Summary]\")\n",
        "    print(f\"Query Similarity: {best_eval['query_similarity']:.3f}\")\n",
        "    print(f\"Context Similarity: {best_eval['context_similarity']:.3f}\")\n",
        "    print(f\"Fluency: {best_eval['fluency']} ({best_eval['fluency_score']:.3f})\")\n",
        "    print(f\"Final Score: {max(score_express, score_premier):.3f}\")\n",
        "\n",
        "    # Save to chat history\n",
        "    chat_history.append({\n",
        "        \"question\": human_input,\n",
        "        \"answer\": best_text\n",
        "    })"
      ],
      "metadata": {
        "id": "UPDD4n4SwOjA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}