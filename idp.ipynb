{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy1DPK/uLwhrZk1ApS10p1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GXzzy1tC8Dt"
      },
      "outputs": [],
      "source": [
        "!pip install boto3 langchain langchain-pinecone langchain-community nltk sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "kApbgSEIDL64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AWS Credentials\n",
        "AWS_ACCESS_KEY_ID=''  # Replace with your AWS Access Key ID\n",
        "AWS_SECRET_ACCESS_KEY=''  # Replace with your AWS Secret Access Key\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_REGION='us-east-1'  # Set the AWS region (default: us-east-1)\n",
        "\n",
        "# Pinecone API Key and Index\n",
        "PINECONE_API_KEY=''  # Replace with your Pinecone API Key\n",
        "PINECONE_INDEX=''  # Replace with your Pinecone Index Name\n",
        "\n",
        "# Amazon S3 Bucket and File Details\n",
        "S3_BUCKET_NAME=''  # Replace with your S3 bucket name where the document is stored\n",
        "PDF_FILE_NAME=''  # Replace with the filename of the document to process\n"
      ],
      "metadata": {
        "id": "cCNuOAArDgoN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AWS Textract client\n",
        "client = boto3.client(\n",
        "    'textract',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "    region_name=AWS_REGION\n",
        ")\n",
        "\n",
        "# Start document text detection\n",
        "response = client.start_document_text_detection(\n",
        "    DocumentLocation={\"S3Object\": {\"Bucket\": S3_BUCKET_NAME, \"Name\": PDF_FILE_NAME}}\n",
        ")\n",
        "job_id = response[\"JobId\"]\n",
        "print(f\"Job started with Job ID: {job_id}\")\n",
        "\n",
        "# Polling for job completion\n",
        "while True:\n",
        "    result = client.get_document_text_detection(JobId=job_id)\n",
        "    status = result[\"JobStatus\"]\n",
        "\n",
        "    if status in [\"SUCCEEDED\", \"FAILED\"]:\n",
        "        break\n",
        "\n",
        "    print(\"Processing...\")\n",
        "    time.sleep(5)\n",
        "\n",
        "if status == \"FAILED\":\n",
        "    raise Exception(\"Textract job failed!\")\n",
        "\n",
        "print(\"Processing completed!\")"
      ],
      "metadata": {
        "id": "xowCj9k7EAqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Text from Response\n",
        "extracted_text = []\n",
        "while True:\n",
        "    if \"Blocks\" in result:\n",
        "        for block in result[\"Blocks\"]:\n",
        "            if block[\"BlockType\"] == \"LINE\" and \"Text\" in block:\n",
        "                extracted_text.append(block[\"Text\"])\n",
        "\n",
        "    if \"NextToken\" in result:\n",
        "        result = client.get_document_text_detection(JobId=job_id, NextToken=result[\"NextToken\"])\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Combine extracted text into a single string\n",
        "full_text = \"\\n\".join(extracted_text)\n",
        "\n",
        "# Save extracted text to a file\n",
        "output_file_name = \"extracted_text.txt\"\n",
        "with open(output_file_name, \"w\") as output_file_io:\n",
        "    output_file_io.write(full_text)\n",
        "\n",
        "# NLP Preprocessing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization\n",
        "    tokens = [word for word in tokens if word.isalnum()]  # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Stopword removal\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "preprocessed_text = preprocess_text(full_text)\n",
        "\n",
        "# Prepare Document for Embedding\n",
        "docs = [Document(page_content=preprocessed_text)]\n",
        "\n",
        "# Split document into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1200, chunk_overlap=250, separator=\"\\n\")\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# Use Embeddings for Text Processing\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "docsearch = PineconeVectorStore.from_documents(split_docs, embedding_model, index_name=PINECONE_INDEX)\n",
        "\n",
        "print(\"Processing complete!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TAr3SGiTGTCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query Processing\n",
        "human_input = input(\"Enter your question: \")\n",
        "query_embedding = embedding_model.embed_query(human_input)  # Convert query into embeddings\n",
        "\n",
        "existing_search = PineconeVectorStore.from_existing_index(index_name=PINECONE_INDEX, embedding=embedding_model)\n",
        "search_results = docsearch.similarity_search(human_input, k=5)\n",
        "\n",
        "# Retrieve relevant context\n",
        "MAX_CONTEXT_LENGTH = 6000\n",
        "context_string = '\\n\\n'.join([f'Document {ind+1}: ' + i.page_content[:MAX_CONTEXT_LENGTH] for ind, i in enumerate(search_results)])\n",
        "\n",
        "# Define RAG Prompt Template\n",
        "RAG_PROMPT_TEMPLATE = '''You are a helpful AI assistant. Use the provided context to answer the question.\n",
        "\n",
        "If the context is insufficient, rely on your own knowledge to provide the best response.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "You are a helpful AI assistant. Use the provided context to answer the question.\n",
        "\n",
        "If the context is insufficient, rely on your own knowledge to provide the best response.\n",
        "\n",
        "Question: {human_input}\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "\n",
        "PROMPT = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
        "prompt_data = PROMPT.format(human_input=human_input, context=context_string)\n",
        "\n",
        "# Query Amazon Titan LLM\n",
        "boto3_bedrock = boto3.client(\n",
        "    'bedrock-runtime',\n",
        "    region_name=AWS_REGION,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")\n",
        "\n",
        "body_part = json.dumps({\n",
        "    'inputText': prompt_data,\n",
        "    'textGenerationConfig': {'maxTokenCount': 8192, 'stopSequences': [], 'temperature': 0.7, 'topP': 1}\n",
        "})\n",
        "\n",
        "response = boto3_bedrock.invoke_model(\n",
        "    body=body_part,\n",
        "    contentType=\"application/json\",\n",
        "    accept=\"application/json\",\n",
        "    modelId='amazon.titan-text-express-v1'\n",
        ")\n",
        "\n",
        "output_text = json.loads(response['body'].read())['results'][0]['outputText']\n",
        "output_text = output_text.replace(\". \", \".\\n\")\n",
        "print(f\"Answer:\\n{output_text}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XthEQVf8HAMS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}