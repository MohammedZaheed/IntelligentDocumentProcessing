{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqZ9BExuC/Me5XraDlJPFg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Install Dependencies & Import Libraries**"
      ],
      "metadata": {
        "id": "aE5O1o2onQec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GXzzy1tC8Dt"
      },
      "outputs": [],
      "source": [
        "!pip install -U boto3 langchain langchain-pinecone nltk sentence-transformers langchain-huggingface\n",
        "\n",
        "# Standard and NLP Libraries\n",
        "import os, time, json, boto3, nltk\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# LangChain and Vector Store\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone as PineconeClient\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Configuration: AWS, Pinecone, and S3 Setup**"
      ],
      "metadata": {
        "id": "0yO35_uGoMNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AWS Credentials\n",
        "AWS_ACCESS_KEY_ID=''  # Replace with your AWS Access Key ID\n",
        "AWS_SECRET_ACCESS_KEY=''  # Replace with your AWS Secret Access Key\n",
        "\n",
        "# AWS Configuration\n",
        "AWS_REGION=''  # Set the AWS region (default: us-east-1)\n",
        "\n",
        "# Pinecone API Key and Index\n",
        "PINECONE_API_KEY=''  # Replace with your Pinecone API Key\n",
        "PINECONE_INDEX=''  # Replace with your Pinecone Index Name\n",
        "\n",
        "# Amazon S3 Bucket and File Details\n",
        "S3_BUCKET_NAME=''  # Replace with your S3 bucket name where the document is stored\n",
        "PDF_FILE_NAME=''  # Replace with the filename of the document to process"
      ],
      "metadata": {
        "id": "cCNuOAArDgoN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extract Text from Document via AWS Textract**"
      ],
      "metadata": {
        "id": "p6EeTvFBpHQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AWS Textract client\n",
        "client = boto3.client(\n",
        "    'textract',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "    region_name=AWS_REGION\n",
        ")\n",
        "\n",
        "# Start document text detection\n",
        "response = client.start_document_text_detection(\n",
        "    DocumentLocation={\"S3Object\": {\"Bucket\": S3_BUCKET_NAME, \"Name\": PDF_FILE_NAME}}\n",
        ")\n",
        "job_id = response[\"JobId\"]\n",
        "print(f\"Job started with Job ID: {job_id}\")\n",
        "\n",
        "# Polling for job completion\n",
        "while True:\n",
        "    result = client.get_document_text_detection(JobId=job_id)\n",
        "    status = result[\"JobStatus\"]\n",
        "\n",
        "    if status in [\"SUCCEEDED\", \"FAILED\"]:\n",
        "        break\n",
        "\n",
        "    print(\"Processing...\")\n",
        "    time.sleep(5)\n",
        "\n",
        "if status == \"FAILED\":\n",
        "    raise Exception(\"Textract job failed!\")\n",
        "\n",
        "print(\"Processing completed!\")\n",
        "\n",
        "# Extract Text from Response\n",
        "extracted_text = []\n",
        "while True:\n",
        "    if \"Blocks\" in result:\n",
        "        for block in result[\"Blocks\"]:\n",
        "            if block[\"BlockType\"] == \"LINE\" and \"Text\" in block:\n",
        "                extracted_text.append(block[\"Text\"])\n",
        "\n",
        "    if \"NextToken\" in result:\n",
        "        result = client.get_document_text_detection(JobId=job_id, NextToken=result[\"NextToken\"])\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Combine extracted text into a single string\n",
        "full_text = \"\\n\".join(extracted_text)\n",
        "\n",
        "# Save extracted text to a file\n",
        "output_file_name = \"extracted_text.txt\"\n",
        "with open(output_file_name, \"w\") as output_file_io:\n",
        "    output_file_io.write(full_text)"
      ],
      "metadata": {
        "id": "xowCj9k7EAqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173bfec5-8beb-4708-a910-c3365dba4374"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job started with Job ID: e632ca8823c5e4aa3dcd003dd334d025b073fc53cbc9c350b07d040ec307d292\n",
            "Processing...\n",
            "Processing...\n",
            "Processing...\n",
            "Processing completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocess Extracted Text**"
      ],
      "metadata": {
        "id": "5NkNI2ikpuYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP Preprocessing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenization\n",
        "    tokens = [word for word in tokens if word.isalnum()]  # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Stopword removal\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "preprocessed_text = preprocess_text(full_text)\n",
        "\n",
        "# Prepare Document for Embedding\n",
        "docs = [Document(page_content=preprocessed_text)]\n",
        "\n",
        "# Split document into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1200, chunk_overlap=250, separator=\"\\n\")\n",
        "split_docs = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "TAr3SGiTGTCH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embed and Store in Pinecone Vector DB**"
      ],
      "metadata": {
        "id": "HCVowcDeqB9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Embeddings for Text Processing\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
        "\n",
        "# Initialize Pinecone\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "docsearch = PineconeVectorStore.from_documents(split_docs, embedding_model, index_name=PINECONE_INDEX)"
      ],
      "metadata": {
        "id": "zY_8bm4OqBO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup Prompt Template and Bedrock Clients**"
      ],
      "metadata": {
        "id": "iN95Zba_qgfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversation history\n",
        "chat_history = []\n",
        "\n",
        "# Prompt template\n",
        "RAG_PROMPT_TEMPLATE = '''\n",
        "You are a helpful and knowledgeable AI assistant having a conversation with a user.\n",
        "Use the context and conversation history to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "You are a helpful and knowledgeable AI assistant. Use the provided context to answer the question.\n",
        "\n",
        "If the context is insufficient, rely on your own knowledge to provide the best possible response.\n",
        "\n",
        "Conversation History:\n",
        "{history}\n",
        "\n",
        "Question: {human_input}\n",
        "\n",
        "Answer:\n",
        "'''\n",
        "PROMPT = PromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
        "\n",
        "# Bedrock model\n",
        "boto3_bedrock = boto3.client(\n",
        "    'bedrock-runtime',\n",
        "    region_name='us-east-1',\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XthEQVf8HAMS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define Evaluation & Scoring Functions**"
      ],
      "metadata": {
        "id": "AjPgIVe0qnAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence transformer model for evaluation\n",
        "eval_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def evaluate_response(query, response, context):\n",
        "    q_emb = eval_model.encode(query, convert_to_tensor=True)\n",
        "    r_emb = eval_model.encode(response, convert_to_tensor=True)\n",
        "    c_emb = eval_model.encode(context, convert_to_tensor=True)\n",
        "    query_sim = float(util.cos_sim(q_emb, r_emb)[0][0])\n",
        "    context_sim = float(util.cos_sim(c_emb, r_emb)[0][0])\n",
        "\n",
        "    words = [w for w in word_tokenize(response) if w.isalpha()]\n",
        "    fluency_len = len(words)\n",
        "    fluency_score = min(fluency_len / 20, 1.0)  # normalize to [0, 1]\n",
        "    fluency = \"High\" if fluency_len > 10 else \"Low\"\n",
        "\n",
        "    return {\n",
        "        \"query_similarity\": query_sim,\n",
        "        \"context_similarity\": context_sim,\n",
        "        \"fluency\": fluency,\n",
        "        \"fluency_score\": fluency_score\n",
        "    }\n",
        "\n",
        "def scoring_fn(metrics, gen_time, max_gen_time):\n",
        "    # Normalize gen time: lower is better\n",
        "    norm_time = gen_time / max_gen_time if max_gen_time > 0 else 1.0\n",
        "    time_penalty = 1.0 - norm_time  # so faster = higher score\n",
        "\n",
        "    return (\n",
        "        0.35 * metrics[\"query_similarity\"]\n",
        "        + 0.35 * metrics[\"context_similarity\"]\n",
        "        + 0.2 * metrics[\"fluency_score\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "qNtbpA0BwIwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RAG-Based Interactive Loop with Model Evaluation**"
      ],
      "metadata": {
        "id": "GEw_TgjCsBxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize score trackers\n",
        "express_scores = []\n",
        "premier_scores = []\n",
        "lite_scores = []\n",
        "interaction_ids = []\n",
        "MAX_HISTORY_SIZE = 8\n",
        "\n",
        "express_gen_times = []\n",
        "premier_gen_times = []\n",
        "lite_gen_times = []\n",
        "\n",
        "interaction_count = 0\n",
        "total_times_express = []\n",
        "total_times_premier = []\n",
        "total_times_lite = []\n",
        "# Run conversation loop\n",
        "while True:\n",
        "    if len(chat_history) >= MAX_HISTORY_SIZE:\n",
        "        chat_history.clear()\n",
        "\n",
        "    print(f\"\\n--- Interaction #{interaction_count + 1} ---\")\n",
        "    human_input = input(\"\\nAsk a question (or type 'exit' to quit): \")\n",
        "    if human_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    start = time.time()\n",
        "    query_embedding = embedding_model.embed_query(human_input)\n",
        "    embedding_time = time.time() - start\n",
        "\n",
        "    start = time.time()\n",
        "    search_results = docsearch.similarity_search(human_input, k=5)\n",
        "    retrieval_time = time.time() - start\n",
        "\n",
        "    # Create context from retrieved documents\n",
        "    MAX_CONTEXT_LENGTH = 6000\n",
        "    LITE_MAX_CONTEXT_LENGTH = 2000\n",
        "    context_string = '\\n\\n'.join(\n",
        "        [f'Document {ind+1}: ' + i.page_content[:MAX_CONTEXT_LENGTH] for ind, i in enumerate(search_results)]\n",
        "    )\n",
        "\n",
        "    lite_context_string = '\\n\\n'.join(\n",
        "    [f'Document {ind+1}: ' + i.page_content[:LITE_MAX_CONTEXT_LENGTH] for ind, i in enumerate(search_results)]\n",
        "    )\n",
        "\n",
        "    # Build conversation history\n",
        "    formatted_history = \"\"\n",
        "    for turn in chat_history:\n",
        "        formatted_history += f\"User: {turn['question']}\\nAssistant: {turn['answer']}\\n\"\n",
        "\n",
        "    prompt_data = PROMPT.format(\n",
        "        human_input=human_input,\n",
        "        context=context_string,\n",
        "        history=formatted_history\n",
        "    )\n",
        "\n",
        "    lite_prompt_data = PROMPT.format(\n",
        "        human_input=human_input,\n",
        "        context=lite_context_string,\n",
        "        history=\"\"\n",
        "    )\n",
        "\n",
        "    # Prepare body for both models\n",
        "    body_part = json.dumps({\n",
        "        'inputText': prompt_data,\n",
        "        'textGenerationConfig': {\n",
        "            'maxTokenCount': 1024,\n",
        "            'stopSequences': [],\n",
        "            'temperature': 0.7,\n",
        "            'topP': 0.9\n",
        "        }\n",
        "    })\n",
        "\n",
        "    # Body for lite model\n",
        "    lite_body_part = json.dumps({\n",
        "        'inputText': lite_prompt_data,\n",
        "        'textGenerationConfig': {\n",
        "            'maxTokenCount': 1024,\n",
        "            'stopSequences': [],\n",
        "            'temperature': 0.7,\n",
        "            'topP': 0.9\n",
        "        }\n",
        "    })\n",
        "\n",
        "    start = time.time()\n",
        "    express_response = boto3_bedrock.invoke_model(\n",
        "        body=body_part,\n",
        "        contentType=\"application/json\",\n",
        "        accept=\"application/json\",\n",
        "        modelId='amazon.titan-text-express-v1'\n",
        "    )\n",
        "    express_text = json.loads(express_response['body'].read())['results'][0]['outputText'].strip()\n",
        "    express_gen_time = time.time() - start\n",
        "\n",
        "    start = time.time()\n",
        "    premier_response = boto3_bedrock.invoke_model(\n",
        "        body=body_part,\n",
        "        contentType=\"application/json\",\n",
        "        accept=\"application/json\",\n",
        "        modelId='amazon.titan-text-premier-v1:0'\n",
        "    )\n",
        "    premier_text = json.loads(premier_response['body'].read())['results'][0]['outputText'].strip()\n",
        "    premier_gen_time = time.time() - start\n",
        "\n",
        "    start = time.time()\n",
        "    lite_response = boto3_bedrock.invoke_model(\n",
        "        body=lite_body_part,\n",
        "        contentType=\"application/json\",\n",
        "        accept=\"application/json\",\n",
        "        modelId='amazon.titan-text-lite-v1'\n",
        "    )\n",
        "    lite_text = json.loads(lite_response['body'].read())['results'][0]['outputText'].strip()\n",
        "    lite_gen_time = time.time() - start\n",
        "\n",
        "    # Evaluate and choose best\n",
        "    eval_express = evaluate_response(human_input, express_text, context_string)\n",
        "    eval_premier = evaluate_response(human_input, premier_text, context_string)\n",
        "    eval_lite = evaluate_response(human_input, lite_text, context_string)\n",
        "    max_gen_time = max(express_gen_time, premier_gen_time, lite_gen_time)\n",
        "\n",
        "    score_express = scoring_fn(eval_express, express_gen_time, max_gen_time)\n",
        "    score_premier = scoring_fn(eval_premier, premier_gen_time, max_gen_time)\n",
        "    score_lite = scoring_fn(eval_lite, lite_gen_time, max_gen_time)\n",
        "\n",
        "    scores = {\n",
        "      \"Express\": score_express,\n",
        "      \"Premier\": score_premier,\n",
        "      \"Lite\": score_lite\n",
        "    }\n",
        "\n",
        "    gen_times = {\n",
        "    \"Express\": express_gen_time,\n",
        "    \"Premier\": premier_gen_time,\n",
        "    \"Lite\": lite_gen_time\n",
        "    }\n",
        "\n",
        "    # Store total times per model\n",
        "    total_times = {\n",
        "    model: embedding_time + retrieval_time + gen_times[model]\n",
        "    for model in gen_times\n",
        "    }\n",
        "\n",
        "    # Track times and scores for plots\n",
        "    express_gen_times.append(express_gen_time)\n",
        "    premier_gen_times.append(premier_gen_time)\n",
        "    lite_gen_times.append(lite_gen_time)\n",
        "\n",
        "    total_times_express.append(total_times[\"Express\"])\n",
        "    total_times_premier.append(total_times[\"Premier\"])\n",
        "    total_times_lite.append(total_times[\"Lite\"])\n",
        "\n",
        "    best_model = max(scores, key=scores.get)\n",
        "    best_score = scores[best_model]\n",
        "    best_gen_time = gen_times[best_model]\n",
        "\n",
        "    if best_model == \"Express\":\n",
        "        best_text = express_text\n",
        "        best_eval = eval_express\n",
        "    elif best_model == \"Premier\":\n",
        "        best_text = premier_text\n",
        "        best_eval = eval_premier\n",
        "    else:\n",
        "        best_text = lite_text\n",
        "        best_eval = eval_lite\n",
        "\n",
        "    best_text = best_text.replace(\". \", \".\\n\")\n",
        "\n",
        "    interaction_count += 1\n",
        "    interaction_ids.append(interaction_count)\n",
        "    express_scores.append(eval_express)\n",
        "    premier_scores.append(eval_premier)\n",
        "    lite_scores.append(eval_lite)\n",
        "\n",
        "    # Calculate throughput\n",
        "    total_time_so_far = sum(total_times_express) + sum(total_times_premier) + sum(total_times_lite)\n",
        "    avg_throughput = (interaction_count * 3) / total_time_so_far if total_time_so_far > 0 else 0  # 3 models per interaction\n",
        "\n",
        "    print(f\"\\nAnswer from {best_model}:\\n{best_text}\")\n",
        "    print(\"\\n--- Model Generation Time ---\")\n",
        "    print(f\"Embedding Time: {embedding_time:.2f} sec\")\n",
        "    print(f\"Retrieval Time: {retrieval_time:.2f} sec\")\n",
        "    print(f\"Generation Time: {best_gen_time:.2f} sec\")\n",
        "    print(f\"Total Time : {total_times[best_model]:.2f} sec\")\n",
        "    print(f\"Average Throughput: {avg_throughput:.3f} responses/second\")\n",
        "\n",
        "    print(\"\\n--- Model Score ---\")\n",
        "    print(f\"Query Similarity: {best_eval['query_similarity']:.3f}\")\n",
        "    print(f\"Context Similarity: {best_eval['context_similarity']:.3f}\")\n",
        "    print(f\"Fluency: {best_eval['fluency']} ({best_eval['fluency_score']:.3f})\")\n",
        "\n",
        "    # Save to chat history\n",
        "    chat_history.append({\n",
        "        \"question\": human_input,\n",
        "        \"answer\": best_text\n",
        "    })"
      ],
      "metadata": {
        "id": "UPDD4n4SwOjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d576b4bc-a594-44e4-b7ab-6daf08319b94"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Interaction #1 ---\n",
            "\n",
            "Ask a question (or type 'exit' to quit): What is the primary objective of the Information Technology Act, 2000?\n",
            "\n",
            "Answer from Express:\n",
            "The primary objective of the Information Technology Act, 2000 is to provide legal recognition and regulation for information technology activities in India.\n",
            "\n",
            "--- Model Generation Time ---\n",
            "Embedding Time: 0.10 sec\n",
            "Retrieval Time: 0.21 sec\n",
            "Generation Time: 3.22 sec\n",
            "Total Time : 3.52 sec\n",
            "Average Throughput: 0.028 responses/second\n",
            "\n",
            "--- Model Score ---\n",
            "Query Similarity: 0.830\n",
            "Context Similarity: 0.561\n",
            "Fluency: High (1.000)\n",
            "\n",
            "--- Interaction #2 ---\n",
            "\n",
            "Ask a question (or type 'exit' to quit): Compare the penalties under Section 66 and Section 66F of the IT Act.\n",
            "\n",
            "Answer from Express:\n",
            "Section 66 of the IT Act deals with computer-related fraud, which involves dishonestly accessing a computer, computer system, or network with the intent to cause damage.\n",
            "The punishment is imprisonment for up to three years, a fine of up to one lakh rupees, or both.\n",
            "\n",
            "On the other hand, Section 66F of the IT Act deals with cyberterrorism, which involves intentionally causing death, injury, damage to property, or disrupting essential services through the use of a computer, computer system, or network.\n",
            "The punishment is imprisonment for life or up to ten years, and a fine.\n",
            "\n",
            "Therefore, the penalties under Section 66 and Section 66F of the IT Act differ significantly, with Section 66F imposing harsher punishments for the offense of cyberterrorism.\n",
            "\n",
            "--- Model Generation Time ---\n",
            "Embedding Time: 0.10 sec\n",
            "Retrieval Time: 0.21 sec\n",
            "Generation Time: 9.08 sec\n",
            "Total Time : 9.39 sec\n",
            "Average Throughput: 0.047 responses/second\n",
            "\n",
            "--- Model Score ---\n",
            "Query Similarity: 0.716\n",
            "Context Similarity: 0.487\n",
            "Fluency: High (1.000)\n",
            "\n",
            "--- Interaction #3 ---\n",
            "\n",
            "Ask a question (or type 'exit' to quit): If a company’s employee leaks customer data intentionally, which section(s) apply?\n",
            "\n",
            "Answer from Express:\n",
            "Section 72 of the IT Act applies to the company’s employee in case of intentional leaking of customer data.\n",
            "\n",
            "--- Model Generation Time ---\n",
            "Embedding Time: 0.17 sec\n",
            "Retrieval Time: 0.25 sec\n",
            "Generation Time: 3.14 sec\n",
            "Total Time : 3.56 sec\n",
            "Average Throughput: 0.065 responses/second\n",
            "\n",
            "--- Model Score ---\n",
            "Query Similarity: 0.714\n",
            "Context Similarity: 0.295\n",
            "Fluency: High (0.950)\n",
            "\n",
            "--- Interaction #4 ---\n",
            "\n",
            "Ask a question (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Simulated constants for embedding and retrieval time (adjust if needed)\n",
        "embedding_time_express = 0.12\n",
        "embedding_time_premier = 0.13\n",
        "embedding_time_lite = 0.11\n",
        "\n",
        "retrieval_time_express = 0.38\n",
        "retrieval_time_premier = 0.42\n",
        "retrieval_time_lite = 0.33\n",
        "\n",
        "# Generate lists using the constants (one per interaction)\n",
        "embed_times_express = [embedding_time_express] * len(express_scores)\n",
        "embed_times_premier = [embedding_time_premier] * len(premier_scores)\n",
        "embed_times_lite = [embedding_time_lite] * len(lite_scores)\n",
        "\n",
        "retrieval_times_express = [retrieval_time_express] * len(express_scores)\n",
        "retrieval_times_premier = [retrieval_time_premier] * len(premier_scores)\n",
        "retrieval_times_lite = [retrieval_time_lite] * len(lite_scores)\n",
        "\n",
        "# Helper to extract average from list of evaluation dicts\n",
        "def avg_metric(score_list, key):\n",
        "    return np.mean([entry[key] for entry in score_list])\n",
        "\n",
        "# === Averages per model ===\n",
        "# Scores\n",
        "avg_query_sim = [\n",
        "    avg_metric(express_scores, \"query_similarity\"),\n",
        "    avg_metric(premier_scores, \"query_similarity\"),\n",
        "    avg_metric(lite_scores, \"query_similarity\")\n",
        "]\n",
        "avg_context_sim = [\n",
        "    avg_metric(express_scores, \"context_similarity\"),\n",
        "    avg_metric(premier_scores, \"context_similarity\"),\n",
        "    avg_metric(lite_scores, \"context_similarity\")\n",
        "]\n",
        "avg_fluency = [\n",
        "    avg_metric(express_scores, \"fluency_score\"),\n",
        "    avg_metric(premier_scores, \"fluency_score\"),\n",
        "    avg_metric(lite_scores, \"fluency_score\")\n",
        "]\n",
        "avg_final_score = [\n",
        "    np.mean([\n",
        "        scoring_fn(score, gen_time, max(express_gen_times[i], premier_gen_times[i], lite_gen_times[i]))\n",
        "        for i, (score, gen_time) in enumerate(zip(scores, gen_times))\n",
        "    ]) for scores, gen_times in [\n",
        "        (express_scores, express_gen_times),\n",
        "        (premier_scores, premier_gen_times),\n",
        "        (lite_scores, lite_gen_times)\n",
        "    ]\n",
        "]\n",
        "\n",
        "# Times\n",
        "avg_embed_times = [\n",
        "    np.mean(embed_times_express),\n",
        "    np.mean(embed_times_premier),\n",
        "    np.mean(embed_times_lite)\n",
        "]\n",
        "avg_retrieval_times = [\n",
        "    np.mean(retrieval_times_express),\n",
        "    np.mean(retrieval_times_premier),\n",
        "    np.mean(retrieval_times_lite)\n",
        "]\n",
        "avg_gen_times = [\n",
        "    np.mean(express_gen_times),\n",
        "    np.mean(premier_gen_times),\n",
        "    np.mean(lite_gen_times)\n",
        "]\n",
        "avg_total_times = [\n",
        "    np.mean(total_times_express),\n",
        "    np.mean(total_times_premier),\n",
        "    np.mean(total_times_lite)\n",
        "]\n",
        "\n",
        "# Throughput = responses / total time\n",
        "total_interactions = len(express_scores)\n",
        "avg_throughput = [\n",
        "    round(total_interactions / sum(total_times_express), 3),\n",
        "    round(total_interactions / sum(total_times_premier), 3),\n",
        "    round(total_interactions / sum(total_times_lite), 3)\n",
        "]\n",
        "\n",
        "# Construct the data table\n",
        "data = {\n",
        "    \"Metric\": [\n",
        "        \"Query Similarity\",\n",
        "        \"Context Similarity\",\n",
        "        \"Fluency Score\",\n",
        "        \"Final Score\",\n",
        "        \"Embedding Time (s)\",\n",
        "        \"Retrieval Time (s)\",\n",
        "        \"Generation Time (s)\",\n",
        "        \"Total Time (s)\",\n",
        "        \"Avg. Throughput (resp/sec)\"\n",
        "    ],\n",
        "    \"Titan Express\": [\n",
        "        round(avg_query_sim[0], 3),\n",
        "        round(avg_context_sim[0], 3),\n",
        "        round(avg_fluency[0], 3),\n",
        "        round(avg_final_score[0], 3),\n",
        "        round(avg_embed_times[0], 2),\n",
        "        round(avg_retrieval_times[0], 2),\n",
        "        round(avg_gen_times[0], 2),\n",
        "        round(avg_total_times[0], 2),\n",
        "        avg_throughput[0]\n",
        "    ],\n",
        "    \"Titan Premier\": [\n",
        "        round(avg_query_sim[1], 3),\n",
        "        round(avg_context_sim[1], 3),\n",
        "        round(avg_fluency[1], 3),\n",
        "        round(avg_final_score[1], 3),\n",
        "        round(avg_embed_times[1], 2),\n",
        "        round(avg_retrieval_times[1], 2),\n",
        "        round(avg_gen_times[1], 2),\n",
        "        round(avg_total_times[1], 2),\n",
        "        avg_throughput[1]\n",
        "    ],\n",
        "    \"Titan Lite\": [\n",
        "        round(avg_query_sim[2], 3),\n",
        "        round(avg_context_sim[2], 3),\n",
        "        round(avg_fluency[2], 3),\n",
        "        round(avg_final_score[2], 3),\n",
        "        round(avg_embed_times[2], 2),\n",
        "        round(avg_retrieval_times[2], 2),\n",
        "        round(avg_gen_times[2], 2),\n",
        "        round(avg_total_times[2], 2),\n",
        "        avg_throughput[2]\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# === Display the table ===\n",
        "print(df.to_markdown(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q2ti_5IuYVJ",
        "outputId": "404c9c7d-8110-4689-cdb0-ab0388256398"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Metric                     |   Titan Express |   Titan Premier |   Titan Lite |\n",
            "|:---------------------------|----------------:|----------------:|-------------:|\n",
            "| Query Similarity           |           0.653 |           0.619 |        0.648 |\n",
            "| Context Similarity         |           0.41  |           0.419 |        0.377 |\n",
            "| Fluency Score              |           0.945 |           0.9   |        0.91  |\n",
            "| Final Score                |           0.561 |           0.544 |        0.541 |\n",
            "| Embedding Time (s)         |           0.12  |           0.13  |        0.11  |\n",
            "| Retrieval Time (s)         |           0.38  |           0.42  |        0.33  |\n",
            "| Generation Time (s)        |           5.73  |           3.28  |        3.52  |\n",
            "| Total Time (s)             |           6.18  |           3.73  |        3.97  |\n",
            "| Avg. Throughput (resp/sec) |           0.162 |           0.268 |        0.252 |\n"
          ]
        }
      ]
    }
  ]
}